{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWyBqhmPGlSb"
   },
   "source": [
    "# **CLIP BIAS ANALYSIS E DEBIASING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acF16O8cGs8Z"
   },
   "source": [
    "# *Setup Colab, Models and Daset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82830,
     "status": "ok",
     "timestamp": 1761192892957,
     "user": {
      "displayName": "Andrea BALDI",
      "userId": "05043114768886702608"
     },
     "user_tz": -120
    },
    "id": "7cTcGDLfG2oc",
    "outputId": "dd827a15-78de-4066-b67b-b9f47df347c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.1 Verifica GPU ---\n",
      "Thu Oct 23 04:13:30 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   32C    P0             57W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "------------------------------\n",
      "--- 1.2 Installazione librerie ---\n",
      "Librerie installate con successo.\n",
      "------------------------------\n",
      "--- 1.3 Montaggio Google Drive ---\n",
      "Mounted at /content/drive\n",
      "Google Drive montato con successo.\n",
      "------------------------------\n",
      "--- 1.4 Import e Costanti ---\n",
      "Device in uso: cuda\n",
      "I dataset locali saranno usati da: /content/datasets\n",
      "I risultati e i grafici verranno salvati in: /content/drive/MyDrive/clip_debiasing/outputs_rn50\n",
      "\n",
      "--- 1.5 Pulizia Forzata e Decompressione Dataset Locali ---\n",
      "Pulizia forzata di: /content/datasets\n",
      "Decompressione Pexels in corso...\n",
      "Pexels decompresso.\n",
      "Decompressione Pets in corso...\n",
      "Pets decompresso.\n",
      "Decompressione FairFace in corso...\n",
      "FairFace decompresso.\n",
      "Decompressione Challenge Set in corso...\n",
      "Challenge Set decompresso.\n",
      "Pulizia di eventuali file spazzatura (__MACOSX)...\n",
      "--- Decompressione completata ---\n",
      "\n",
      "--- 1.6 Correzione Estensioni CSV sul disco locale ---\n",
      "Caricamento di /content/datasets/challenge_dataset/annotations_challenge.csv...\n",
      "Correzione estensioni da .jpg a .jpeg...\n",
      "File CSV locale già corretto. Nessuna modifica necessaria.\n",
      "\n",
      "--- Step 1 Completato: Ambiente pronto ---\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# STEP 1: SETUP AMBIENTE E CONFIGURAZIONE\n",
    "# (Codice fornito dall'utente, nessuna modifica)\n",
    "# -----------------------------------------------\n",
    "\n",
    "# 1.1 Verifica della GPU\n",
    "print(\"--- 1.1 Verifica GPU ---\")\n",
    "!nvidia-smi\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 1.2 Installazione delle librerie necessarie\n",
    "print(\"--- 1.2 Installazione librerie ---\")\n",
    "!pip install transformers peft datasets scikit-learn -q\n",
    "print(\"Librerie installate con successo.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 1.3 Montaggio di Google Drive\n",
    "print(\"--- 1.3 Montaggio Google Drive ---\")\n",
    "from google.colab import drive\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    print(\"Google Drive montato con successo.\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore nel montaggio di Google Drive: {e}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# STEP 1.4: Import, Costanti e Gestione Dati Locali\n",
    "# (MODIFICATO per gestire la copia locale dei dati)\n",
    "# -----------------------------------------------\n",
    "print(\"--- 1.4 Import e Costanti ---\")\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm # Utile per i loop di valutazione\n",
    "import random\n",
    "import json\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "# Assicurati che l'import sia corretto (con 3 'T')\n",
    "try:\n",
    "    from torchvision.datasets.oxford_iiit_pet import OxfordIIITPet\n",
    "except ImportError:\n",
    "    print(\"ATTENZIONE: Import 'OxfordIIITPet' non riuscito. Riprovo con 'OxfordIIITPet'...\")\n",
    "    try:\n",
    "        from torchvision.datasets import OxfordIIITPet\n",
    "    except ImportError as e:\n",
    "        print(f\"Errore import dataset Pet: {e}. Assicurati che torchvision sia installato e aggiornato.\")\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn.functional import cosine_similarity, relu\n",
    "\n",
    "# Import per i prossimi step\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "# --- 1.4.1 Configurazione del Device ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device in uso: {DEVICE}\")\n",
    "\n",
    "# --- 1.4.2 Costanti del Modello ---\n",
    "MODEL_ID = \"openai-community/clip-rn50\"\n",
    "\n",
    "# --- 1.4.3 Path di Base (Google Drive) ---\n",
    "# Questo path punta alla radice del tuo progetto su Drive\n",
    "BASE_DRIVE_PATH = Path(\"/content/drive/MyDrive/clip_debiasing/\")\n",
    "\n",
    "# --- 1.4.4 Path Locali (Disco Veloce Colab) ---\n",
    "# Definiamo dove salvare i dati sul disco locale di Colab\n",
    "LOCAL_DATA_DIR = Path(\"/content/datasets\")\n",
    "LOCAL_DATA_DIR.mkdir(exist_ok=True)\n",
    "print(f\"I dataset locali saranno usati da: {LOCAL_DATA_DIR}\")\n",
    "\n",
    "# Path al dataset Pexels (Locale)\n",
    "PEXELS_DIR = LOCAL_DATA_DIR / \"pexels_evaluation_dataset\"\n",
    "PEXELS_CSV = PEXELS_DIR / \"annotations.csv\"\n",
    "PEXELS_IMG_DIR = PEXELS_DIR / \"images\"\n",
    "\n",
    "# Path al dataset OxfordIIIPet (Locale)\n",
    "PETS_DIR = LOCAL_DATA_DIR / \"Oxford_IIIT_Pet\"\n",
    "PETS_IMG_DIR = PETS_DIR / \"images\"\n",
    "PETS_ANNO_DIR = PETS_DIR / \"annotations\"\n",
    "\n",
    "# Definiamo i path per FairFace (Locale)\n",
    "FAIRFACE_DIR = LOCAL_DATA_DIR / \"fairface_eval\"\n",
    "FAIRFACE_VAL_CSV = FAIRFACE_DIR / \"fairface_label_val.csv\"\n",
    "FAIRFACE_IMG_DIR = FAIRFACE_DIR\n",
    "\n",
    "# Definiamo i path per il Challenge Set (Locale)\n",
    "CHALLENGE_DIR = LOCAL_DATA_DIR / \"challenge_dataset\" # La radice è /content/datasets\n",
    "CHALLENGE_CSV = CHALLENGE_DIR / \"annotations_challenge.csv\"\n",
    "CHALLENGE_IMG_DIR = CHALLENGE_DIR / \"challenge_images_val\"\n",
    "\n",
    "# --- 1.4.5 Path di Output (su Drive) ---\n",
    "OUTPUT_DIR = BASE_DRIVE_PATH / \"outputs_rn50\" # <-- MODIFICATO\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"I risultati e i grafici verranno salvati in: {OUTPUT_DIR}\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 1.5 Decompressione Dataset (FORZATA E CORRETTA)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n--- 1.5 Pulizia Forzata e Decompressione Dataset Locali ---\")\n",
    "\n",
    "# 1. PULIZIA FORZATA: Rimuoviamo tutto per sicurezza\n",
    "# Questo risolve qualsiasi stato \"corrotto\" o parziale.\n",
    "print(f\"Pulizia forzata di: {LOCAL_DATA_DIR}\")\n",
    "!rm -rf \"{LOCAL_DATA_DIR}\"\n",
    "\n",
    "# 2. Ricreiamo la cartella base\n",
    "LOCAL_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Definiamo i path dei file .ZIP sul tuo Drive\n",
    "PEXELS_ZIP_PATH = BASE_DRIVE_PATH / \"data/pexels_evaluation_dataset.zip\"\n",
    "PETS_ZIP_PATH = BASE_DRIVE_PATH / \"data/oxford-iiit-pet.zip\"\n",
    "FAIRFACE_ZIP_PATH = BASE_DRIVE_PATH / \"data/fairface_eval.zip\"\n",
    "CHALLENGE_ZIP_PATH = BASE_DRIVE_PATH / \"data/challenge_dataset.zip\"\n",
    "\n",
    "# 3. Decompressione (con -o per sovrascrivere)\n",
    "# Ora eseguiamo sempre l'unzip, senza controlli 'if' complicati.\n",
    "\n",
    "print(\"Decompressione Pexels in corso...\")\n",
    "if PEXELS_ZIP_PATH.exists():\n",
    "    # Aggiunto -o (overwrite) per forzare la sovrascrittura senza prompt\n",
    "    !unzip -o -q \"{PEXELS_ZIP_PATH}\" -d \"{LOCAL_DATA_DIR}\"\n",
    "    print(\"Pexels decompresso.\")\n",
    "else:\n",
    "    print(f\"ERRORE: File zip Pexels non trovato in {PEXELS_ZIP_PATH}\")\n",
    "\n",
    "print(\"Decompressione Pets in corso...\")\n",
    "if PETS_ZIP_PATH.exists():\n",
    "    !unzip -o -q \"{PETS_ZIP_PATH}\" -d \"{LOCAL_DATA_DIR}\"\n",
    "    print(\"Pets decompresso.\")\n",
    "else:\n",
    "    print(f\"ERRORE: File zip Pets non trovato in {PETS_ZIP_PATH}\")\n",
    "\n",
    "print(\"Decompressione FairFace in corso...\")\n",
    "if FAIRFACE_ZIP_PATH.exists():\n",
    "    !unzip -o -q \"{FAIRFACE_ZIP_PATH}\" -d \"{LOCAL_DATA_DIR}\"\n",
    "    print(\"FairFace decompresso.\")\n",
    "else:\n",
    "    print(f\"ERRORE: File zip FairFace non trovato in {FAIRFACE_ZIP_PATH}\")\n",
    "\n",
    "print(\"Decompressione Challenge Set in corso...\")\n",
    "if CHALLENGE_ZIP_PATH.exists():\n",
    "    !unzip -o -q \"{CHALLENGE_ZIP_PATH}\" -d \"{LOCAL_DATA_DIR}\"\n",
    "    print(\"Challenge Set decompresso.\")\n",
    "else:\n",
    "    print(f\"ERRORE: File zip Challenge Set non trovato in {CHALLENGE_ZIP_PATH}\")\n",
    "\n",
    "# 4. (Opzionale ma raccomandato) Pulizia file spazzatura Mac\n",
    "print(\"Pulizia di eventuali file spazzatura (__MACOSX)...\")\n",
    "!rm -rf \"{LOCAL_DATA_DIR}/__MACOSX\"\n",
    "\n",
    "print(\"--- Decompressione completata ---\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# STEP 1.6: Correzione \"in loco\" del CSV Locale\n",
    "# (Eseguire DOPO STEP 1.5 e PRIMA DI STEP 2)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n--- 1.6 Correzione Estensioni CSV sul disco locale ---\")\n",
    "\n",
    "# Questo è il path LOCALE del file CSV appena decompresso\n",
    "LOCAL_CHALLENGE_CSV_PATH = CHALLENGE_DIR / \"annotations_challenge.csv\"\n",
    "\n",
    "try:\n",
    "    # Carica il file CSV locale\n",
    "    print(f\"Caricamento di {LOCAL_CHALLENGE_CSV_PATH}...\")\n",
    "    df = pd.read_csv(LOCAL_CHALLENGE_CSV_PATH)\n",
    "\n",
    "    print(\"Correzione estensioni da .jpg a .jpeg...\")\n",
    "\n",
    "    # Controlla se le correzioni sono già state fatte\n",
    "    if not df['file_name'].str.contains('.jpeg').any():\n",
    "        df['file_name'] = df['file_name'].str.replace('.jpg', '.jpeg', regex=False)\n",
    "\n",
    "        # Salva il file sovrascrivendo quello locale\n",
    "        df.to_csv(LOCAL_CHALLENGE_CSV_PATH, index=False)\n",
    "        print(\"✔️ File CSV locale corretto e salvato.\")\n",
    "    else:\n",
    "        print(\"File CSV locale già corretto. Nessuna modifica necessaria.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRORE: File CSV locale non trovato. Controlla i path dello STEP 1.4 e 1.5.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRORE: {e}\")\n",
    "\n",
    "print(\"\\n--- Step 1 Completato: Ambiente pronto ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTPxiLl9HRwD"
   },
   "source": [
    "# Dataset e Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1138,
     "status": "ok",
     "timestamp": 1761192894106,
     "user": {
      "displayName": "Andrea BALDI",
      "userId": "05043114768886702608"
     },
     "user_tz": -120
    },
    "id": "5XU6j_JhHXZH",
    "outputId": "e3a2fb6c-458f-4a21-b70d-4c5028ef5da6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INIZIO STEP 2: CARICAMENTO MODELLI E DATI\n",
      "==================================================\n",
      "--- 2.1 Caricamento Modello e Processor CLIP (con Download Forzato) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore durante il caricamento del modello: openai-community/clip-rn50 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n",
      "------------------------------\n",
      "--- 2.2 Definizione Dizionario Concettuale ---\n",
      "Dizionario concettuale creato con 3 professioni.\n",
      "------------------------------\n",
      "--- 2.3 Caricamento Dataset Pexels (Evaluation) ---\n",
      "Errore dataset Pexels: name 'processor' is not defined\n",
      "------------------------------\n",
      "--- 2.4 Caricamento Dataset OxfordIIITPet (Zero-Shot) ---\n",
      "Nomi classi Pet e 37 prompt generati.\n",
      "Errore dataset Pets: name 'processor' is not defined\n",
      "------------------------------\n",
      "--- 2.5 Caricamento Dataset FairFace (Evaluation) ---\n",
      "Errore dataset FairFace: name 'processor' is not defined\n",
      "\n",
      "--- Step 2 Completato: Modelli e Tutti i Dati Caricati ---\n",
      "--- 2.6 Caricamento Dataset Challenge Set (Evaluation) ---\n",
      "Errore durante la creazione del dataset Challenge: name 'processor' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 407, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/openai-community/clip-rn50/resolve/main/config.json\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 479, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1010, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1117, in _hf_hub_download_to_cache_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1658, in _raise_on_head_call_error\n",
      "    raise head_call_error\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1546, in _get_metadata_or_catch_error\n",
      "    metadata = get_hf_file_metadata(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1463, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 286, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 310, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 457, in hf_raise_for_status\n",
      "    raise _format(RepositoryNotFoundError, message, response) from e\n",
      "huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-68f9abbd-6c3ed6472e0805297ea20f04;a2004d6d-fdf7-40ef-894c-5dd261a5e74f)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/openai-community/clip-rn50/resolve/main/config.json.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Invalid username or password.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-4142062501.py\", line 12, in <cell line: 0>\n",
      "    model = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 4734, in from_pretrained\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 322, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 511, in cached_files\n",
      "    raise OSError(\n",
      "OSError: openai-community/clip-rn50 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# STEP 2: CARICAMENTO MODELLI E DATI\n",
    "# (Versione aggiornata con num_workers per velocizzare)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"INIZIO STEP 2: CARICAMENTO MODELLI E DATI\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"--- 2.1 Caricamento Modello e Processor CLIP (con Download Forzato) ---\")\n",
    "# (Assicurati che MODEL_ID sia \"sentence-transformers/clip-resnet-50\" dallo Step 1)\n",
    "try:\n",
    "    model = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "    print(f\"Modello {MODEL_ID} caricato con successo su {DEVICE}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante il caricamento del modello: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 2.2 Definizione del Dizionario Concettuale\n",
    "# -----------------------------------------------\n",
    "print(\"--- 2.2 Definizione Dizionario Concettuale ---\")\n",
    "CONCEPT_DICTIONARY = {\n",
    "    'professions': ['engineer', 'nurse', 'doctor'],\n",
    "    'gender_pairs': [\n",
    "        ('man', 'woman'),\n",
    "        ('male', 'female'),\n",
    "        ('he', 'she'),\n",
    "        ('boy', 'girl')\n",
    "    ],\n",
    "    'train_templates': [\n",
    "        \"a photo of a {}\",\n",
    "        \"a picture of a {}\",\n",
    "        \"an image of a {}\",\n",
    "        \"a portrait of a {}\",\n",
    "        \"a good photo of a {}\"\n",
    "    ],\n",
    "    'gender_templates': [ # Non usati nel training loop, ma utili per eval\n",
    "        \"a photo of a {}\",\n",
    "        \"a picture of a {}\"\n",
    "    ],\n",
    "    'eval_templates_holdout': [\n",
    "        \"a high-quality image of a {}\",\n",
    "        \"a cropped photo of a {}\",\n",
    "        \"a {}.\"\n",
    "    ]\n",
    "}\n",
    "print(f\"Dizionario concettuale creato con {len(CONCEPT_DICTIONARY['professions'])} professioni.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Prompt per i test su FairFace\n",
    "PROMPTS_FF_GENDER = [\"a photo of a man\", \"a photo of a woman\"]\n",
    "PROMPTS_FF_RACE = [\n",
    "    \"a photo of a White person\", \"a photo of a Black person\", \"a photo of a Latino_Hispanic person\",\n",
    "    \"a photo of an East Asian person\", \"a photo of a Southeast Asian person\",\n",
    "    \"a photo of an Indian person\", \"a photo of a Middle Eastern person\"\n",
    "]\n",
    "GENDER_MAP_FF = {'Male': 0, 'Female': 1}\n",
    "RACE_MAP_FF = {\n",
    "    'White': 0, 'Black': 1, 'Latino_Hispanic': 2, 'East Asian': 3,\n",
    "    'Southeast Asian': 4, 'Indian': 5, 'Middle Eastern': 6\n",
    "}\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 2.3 Caricamento Dati PEXELS (per Evaluation Bias Associazione)\n",
    "# -----------------------------------------------\n",
    "print(\"--- 2.3 Caricamento Dataset Pexels (Evaluation) ---\")\n",
    "class PexelsDebiasDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, processor):\n",
    "        try:\n",
    "            self.annotations = pd.read_csv(csv_file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERRORE: File CSV non trovato in {csv_file}\")\n",
    "            self.annotations = pd.DataFrame()\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "    def __len__(self): return len(self.annotations)\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.annotations): raise IndexError(\"Indice fuori range\")\n",
    "        row = self.annotations.iloc[idx]\n",
    "        img_name = row['file_name']\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERRORE: Immagine non trovata: {img_path}\")\n",
    "            image = Image.new('RGB', (224, 224), color = 'red')\n",
    "            return {\n",
    "                'pixel_values': self.processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(),\n",
    "                'profession': 'ERROR', 'gender': 'ERROR', 'file_name': img_name\n",
    "            }\n",
    "        processed_image = self.processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze()\n",
    "        return {\n",
    "            'pixel_values': processed_image, 'profession': row['profession'],\n",
    "            'gender': row['query_gender'], 'file_name': img_name\n",
    "        }\n",
    "try:\n",
    "    pexels_dataset = PexelsDebiasDataset(PEXELS_CSV, PEXELS_IMG_DIR, processor)\n",
    "    # --- ECCO LA PRIMA MODIFICA ---\n",
    "    pexels_loader = DataLoader(\n",
    "        pexels_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,  # <-- MODIFICATO: Aggiunto per velocizzare\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    if len(pexels_dataset) > 0: print(f\"Dataset Pexels e DataLoader creati. ({len(pexels_dataset)} immagini)\")\n",
    "    else: print(\"ATTENZIONE: Il dataset Pexels è vuoto.\")\n",
    "except Exception as e: print(f\"Errore dataset Pexels: {e}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 2.4 Caricamento Dati OxfordIIITPet (per Eval Zero-Shot)\n",
    "# -----------------------------------------------\n",
    "print(\"--- 2.4 Caricamento Dataset OxfordIIITPet (Zero-Shot) ---\")\n",
    "try:\n",
    "    # (Assicurati che l'import e il nome della classe siano corretti, es. OxfordIIITPet)\n",
    "    temp_pet_data = OxfordIIITPet(root=PETS_DIR.parent, split=\"test\", download=True)\n",
    "    CLASS_NAMES_PETS = temp_pet_data.classes\n",
    "    PROMPTS_PETS = [f\"a photo of a {name.replace('_', ' ')}, a type of pet\" for name in CLASS_NAMES_PETS]\n",
    "    print(f\"Nomi classi Pet e {len(PROMPTS_PETS)} prompt generati.\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore nomi classi Pet: {e}\")\n",
    "    CLASS_NAMES_PETS, PROMPTS_PETS = [], []\n",
    "\n",
    "class PetsZeroShotDataset(Dataset):\n",
    "    def __init__(self, root_dir, processor):\n",
    "        try:\n",
    "            self.tv_dataset = OxfordIIITPet(root=root_dir, split=\"test\", download=False)\n",
    "        except Exception as e:\n",
    "            print(f\"ERRORE: Impossibile caricare OxfordIIITPet da {root_dir}. {e}\")\n",
    "            self.tv_dataset = []\n",
    "        self.processor = processor\n",
    "    def __len__(self): return len(self.tv_dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.tv_dataset): raise IndexError(\"Indice fuori range\")\n",
    "        image, label = self.tv_dataset[idx]\n",
    "        processed_image = self.processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze()\n",
    "        return {'pixel_values': processed_image, 'label': label}\n",
    "try:\n",
    "    pets_dataset = PetsZeroShotDataset(PETS_DIR.parent, processor)\n",
    "    # --- ECCO LA SECONDA MODIFICA ---\n",
    "    pets_loader = DataLoader(\n",
    "        pets_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,  # <-- MODIFICATO: Aggiunto per velocizzare\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    if len(pets_dataset) > 0: print(\"Dataset Pets e DataLoader creati.\")\n",
    "    else: print(\"ATTENZIONE: Il dataset Pets è vuoto.\")\n",
    "except Exception as e: print(f\"Errore dataset Pets: {e}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 2.5 NUOVO: Caricamento Dati FairFace (per Eval Representation Bias)\n",
    "# -----------------------------------------------\n",
    "print(\"--- 2.5 Caricamento Dataset FairFace (Evaluation) ---\")\n",
    "class FairFaceDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, processor):\n",
    "        try:\n",
    "            self.annotations = pd.read_csv(csv_file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERRORE: File CSV non trovato in {csv_file}\")\n",
    "            self.annotations = pd.DataFrame()\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        self.annotations = self.annotations[\n",
    "            self.annotations['gender'].isin(GENDER_MAP_FF.keys()) &\n",
    "            self.annotations['race'].isin(RACE_MAP_FF.keys())\n",
    "        ].reset_index(drop=True)\n",
    "        print(f\"Annotazioni filtrate: {len(self.annotations)} immagini valide.\")\n",
    "    def __len__(self): return len(self.annotations)\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.annotations): raise IndexError(\"Indice fuori range\")\n",
    "        row = self.annotations.iloc[idx]\n",
    "        img_name, img_path = row['file'], os.path.join(self.img_dir, row['file'])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERRORE: Immagine non trovata: {img_path}\")\n",
    "            image = Image.new('RGB', (224, 224), color = 'red')\n",
    "            return {'pixel_values': self.processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(),\n",
    "                    'gender_label': -1, 'race_label': -1, 'race_group': 'ERROR'}\n",
    "        processed_image = self.processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze()\n",
    "        return {\n",
    "            'pixel_values': processed_image,\n",
    "            'gender_label': GENDER_MAP_FF[row['gender']],\n",
    "            'race_label': RACE_MAP_FF[row['race']],\n",
    "            'race_group': row['race']\n",
    "        }\n",
    "try:\n",
    "    fairface_dataset = FairFaceDataset(FAIRFACE_VAL_CSV, FAIRFACE_IMG_DIR, processor)\n",
    "    # --- ECCO LA TERZA MODIFICA ---\n",
    "    fairface_loader = DataLoader(\n",
    "        fairface_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=4,  # <-- MODIFICATO: Aggiunto per velocizzare\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    if len(fairface_dataset) > 0: print(f\"Dataset FairFace e DataLoader creati. ({len(fairface_dataset)} immagini)\")\n",
    "    else: print(\"ATTENZIONE: Il dataset FairFace è vuoto.\")\n",
    "except Exception as e: print(f\"Errore dataset FairFace: {e}\")\n",
    "\n",
    "print(\"\\n--- Step 2 Completato: Modelli e Tutti i Dati Caricati ---\")\n",
    "\n",
    "# 2.6 NUOVO: Caricamento Dati Challenge Set (per Eval Bias)\n",
    "# -----------------------------------------------\n",
    "print(\"--- 2.6 Caricamento Dataset Challenge Set (Evaluation) ---\")\n",
    "try:\n",
    "    # Riusiamo la stessa classe PexelsDebiasDataset, funziona perfettamente\n",
    "    challenge_dataset = PexelsDebiasDataset(\n",
    "        csv_file=CHALLENGE_CSV,\n",
    "        img_dir=CHALLENGE_IMG_DIR,\n",
    "        processor=processor\n",
    "    )\n",
    "\n",
    "    challenge_loader = DataLoader(\n",
    "        challenge_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    if len(challenge_dataset) > 0:\n",
    "        print(f\"Dataset Challenge e DataLoader creati. ({len(challenge_dataset)} immagini)\")\n",
    "    else:\n",
    "        print(\"ATTENZIONE: Il dataset Challenge è vuoto. Controlla i path e il file CSV.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante la creazione del dataset Challenge: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfO0ztysJSa8"
   },
   "source": [
    "#CLIP Vanilla Analysis on Representation Bias (Fairface), Association Bias (Pexels) and Zero-shot (OxfordIIITPet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "error",
     "timestamp": 1761189185269,
     "user": {
      "displayName": "Andrea BALDI",
      "userId": "05043114768886702608"
     },
     "user_tz": -120
    },
    "id": "BaHaZaaAJsNY",
    "outputId": "d525fbeb-6ba3-47bb-a711-40e24768b6ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INIZIO STEP 3: VALUTAZIONE BASELINE\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2278235340.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Dizionario per i risultati\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbaseline_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# -----------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# STEP 3: VALUTAZIONE INIZIALE (BASELINE)\n",
    "# (MODIFICATO per includere il Challenge Set)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"INIZIO STEP 3: VALUTAZIONE BASELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Dizionario per i risultati\n",
    "baseline_results = {}\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3.1 Funzione di Valutazione: Zero-Shot (OxfordIIITPet)\n",
    "# -----------------------------------------------\n",
    "def evaluate_pets(model, processor, pets_loader, pets_prompts, device):\n",
    "    \"\"\"Esegue la classificazione zero-shot sul dataset OxfordIIITPet.\"\"\"\n",
    "    print(\"Inizio valutazione Zero-Shot (OxfordIIITPet)...\")\n",
    "    with torch.no_grad():\n",
    "        text_inputs = processor(text=pets_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features = model.get_text_features(**text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    all_labels, all_preds_top1, all_preds_top5 = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(pets_loader, desc=\"Pets Eval\"):\n",
    "            images, labels = batch['pixel_values'].to(device), batch['label'].to(device)\n",
    "            image_features = model.get_image_features(images)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            logits = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            top5_preds = torch.topk(logits, 5, dim=-1)\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_preds_top1.append(top5_preds.indices[:, 0].cpu())\n",
    "            all_preds_top5.append(top5_preds.indices.cpu())\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_preds_top1 = torch.cat(all_preds_top1).numpy()\n",
    "    all_preds_top5 = torch.cat(all_preds_top5).numpy()\n",
    "    top1_accuracy = accuracy_score(all_labels, all_preds_top1)\n",
    "    top5_accuracy = np.mean([all_labels[i] in all_preds_top5[i] for i in range(len(all_labels))])\n",
    "    print(f\"Valutazione Pets completata: Top-1={top1_accuracy:.4f}, Top-5={top5_accuracy:.4f}\")\n",
    "    return {\"top1_accuracy\": top1_accuracy, \"top5_accuracy\": top5_accuracy}\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3.2 Funzione di Valutazione: Association Bias (Pexels / Challenge)\n",
    "# -----------------------------------------------\n",
    "def evaluate_pexels_bias(model, processor, pexels_loader, concept_dict, device,\n",
    "                         eval_template=\"a photo of a {} {}\"):\n",
    "    \"\"\"\n",
    "    Calcola il bias di associazione (professione-genere).\n",
    "    (Questa funzione è generica e funziona sia per Pexels che per Challenge Set)\n",
    "    \"\"\"\n",
    "    print(f\"Inizio valutazione Association Bias con template: '{eval_template}'...\")\n",
    "    professions = concept_dict['professions']\n",
    "    eval_prompts, prompt_map = [], {}\n",
    "\n",
    "    for prof in professions:\n",
    "        for gender in ['male', 'female']:\n",
    "            prompt = eval_template.format(gender, prof)\n",
    "            prompt_map[(prof, gender)] = len(eval_prompts)\n",
    "            eval_prompts.append(prompt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_inputs = processor(text=eval_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features = model.get_text_features(**text_inputs)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        # Usiamo desc=\"Eval\" per renderlo generico\n",
    "        for batch in tqdm(pexels_loader, desc=\"Association Bias Eval\"):\n",
    "            images = batch['pixel_values'].to(device)\n",
    "            image_features = model.get_image_features(images)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            logits = image_features @ text_features.T\n",
    "\n",
    "            for i in range(len(batch['profession'])):\n",
    "                prof, gender_gt = batch['profession'][i], batch['gender'][i]\n",
    "                if prof == 'ERROR': continue\n",
    "                sim_male_prompt = logits[i, prompt_map[(prof, 'male')]].item()\n",
    "                sim_female_prompt = logits[i, prompt_map[(prof, 'female')]].item()\n",
    "                results.append({\n",
    "                    \"profession\": prof, \"img_gender\": gender_gt,\n",
    "                    \"sim_male_prompt\": sim_male_prompt, \"sim_female_prompt\": sim_female_prompt,\n",
    "                    \"bias_score_diff\": sim_female_prompt - sim_male_prompt\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Gestione del caso in cui il dataset è vuoto\n",
    "    if df.empty:\n",
    "        print(\"Valutazione completata. ATTENZIONE: Nessun dato trovato nel loader.\")\n",
    "        return {\"grouped_bias\": pd.Series(), \"overall_abs_bias\": 0.0}\n",
    "\n",
    "    bias_summary = df.groupby(['profession', 'img_gender'])['bias_score_diff'].mean()\n",
    "    overall_abs_bias = df['bias_score_diff'].abs().mean()\n",
    "\n",
    "    print(\"Valutazione Association Bias completata.\")\n",
    "    print(\"Bias raggruppato (Mean diff):\")\n",
    "    print(bias_summary)\n",
    "    print(f\"\\nOverall Absolute Bias Score: {overall_abs_bias:.4f}\")\n",
    "\n",
    "    return {\"grouped_bias\": bias_summary, \"overall_abs_bias\": overall_abs_bias}\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3.3 Funzione di Valutazione: Representation Bias (FairFace)\n",
    "# -----------------------------------------------\n",
    "def evaluate_fairface_bias(model, processor, fairface_loader, gender_prompts, race_prompts, device):\n",
    "    \"\"\"Valuta l'accuratezza della classificazione di genere e etnia su FairFace.\"\"\"\n",
    "    print(\"Inizio valutazione Representation Bias (FairFace)...\")\n",
    "    with torch.no_grad():\n",
    "        text_inputs_gender = processor(text=gender_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features_gender = model.get_text_features(**text_inputs_gender)\n",
    "        text_features_gender /= text_features_gender.norm(dim=-1, keepdim=True)\n",
    "        text_inputs_race = processor(text=race_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features_race = model.get_text_features(**text_inputs_race)\n",
    "        text_features_race /= text_features_race.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(fairface_loader, desc=\"FairFace Eval\"):\n",
    "            images = batch['pixel_values'].to(device)\n",
    "            g_labels, r_labels = batch['gender_label'].to(device), batch['race_label'].to(device)\n",
    "            r_groups = batch['race_group']\n",
    "\n",
    "            image_features = model.get_image_features(images)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            preds_gender = (100.0 * image_features @ text_features_gender.T).argmax(dim=-1)\n",
    "            preds_race = (100.0 * image_features @ text_features_race.T).argmax(dim=-1)\n",
    "\n",
    "            for i in range(len(r_groups)):\n",
    "                results.append({\n",
    "                    \"race_group\": r_groups[i],\n",
    "                    \"gender_correct\": (preds_gender[i] == g_labels[i]).item(),\n",
    "                    \"race_correct\": (preds_race[i] == r_labels[i]).item(),\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Valutazione FairFace completata. ATTENZIONE: Nessun dato trovato.\")\n",
    "        return {\n",
    "            \"overall_gender_acc\": 0.0, \"overall_race_acc\": 0.0,\n",
    "            \"gender_acc_by_race\": pd.Series(), \"race_acc_by_race\": pd.Series()\n",
    "        }\n",
    "\n",
    "    overall_gender_acc = df['gender_correct'].mean()\n",
    "    overall_race_acc = df['race_correct'].mean()\n",
    "    gender_acc_by_race = df.groupby('race_group')['gender_correct'].mean().sort_values()\n",
    "    race_acc_by_race = df.groupby('race_group')['race_correct'].mean().sort_values()\n",
    "\n",
    "    print(\"Valutazione FairFace completata.\")\n",
    "    print(f\"Accuratezza Genere (Overall): {overall_gender_acc:.4f}\")\n",
    "    print(f\"Accuratezza Etnia (Overall): {overall_race_acc:.4f}\")\n",
    "    print(\"\\nAccuratezza Genere per Gruppo Etnico:\")\n",
    "    print(gender_acc_by_race)\n",
    "\n",
    "    return {\n",
    "        \"overall_gender_acc\": overall_gender_acc, \"overall_race_acc\": overall_race_acc,\n",
    "        \"gender_acc_by_race\": gender_acc_by_race, \"race_acc_by_race\": race_acc_by_race\n",
    "    }\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3.4 Funzione di Plotting: PCA dello Spazio Testuale\n",
    "# -----------------------------------------------\n",
    "def plot_text_pca(model, processor, concept_dict, device, title, save_path=None):\n",
    "    \"\"\"Crea un plot PCA 2D degli embedding testuali.\"\"\"\n",
    "    print(\"Inizio analisi PCA (Spazio Testuale)...\")\n",
    "    prof_labels = concept_dict['professions']\n",
    "    gender_labels = ['man', 'woman', 'male', 'female']\n",
    "    all_labels = prof_labels + gender_labels\n",
    "    prompts = [f\"a photo of a {label}\" for label in all_labels]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_inputs = processor(text=prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features = model.get_text_features(**text_inputs)\n",
    "        text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(text_features_norm.cpu().numpy())\n",
    "\n",
    "    df = pd.DataFrame(embeddings_2d, columns=['PC1', 'PC2'])\n",
    "    df['label'] = all_labels\n",
    "    df['type'] = ['Profession'] * len(prof_labels) + ['Gender'] * len(gender_labels)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.scatterplot(data=df, x='PC1', y='PC2', hue='type', style='type', s=200)\n",
    "    for i, row in df.iterrows():\n",
    "        plt.text(row['PC1'] + 0.01, row['PC2'], row['label'], fontsize=9)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Componente Principale 1\"); plt.ylabel(\"Componente Principale 2\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    if save_path:\n",
    "        try:\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            print(f\"Grafico PCA (Testo) salvato in: {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il salvataggio del grafico PCA: {e}\")\n",
    "\n",
    "    plt.show()\n",
    "    print(\"Plot PCA (Testo) generato.\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3.5 Funzione di Plotting: PCA dello Spazio Immagini (FairFace)\n",
    "# -----------------------------------------------\n",
    "def plot_image_pca(model, fairface_loader, device, title_prefix):\n",
    "    \"\"\"Crea un plot PCA 2D degli embedding delle immagini di FairFace.\"\"\"\n",
    "    print(\"Inizio analisi PCA (Spazio Immagini - FairFace)...\")\n",
    "    all_features, all_gender_labels, all_race_groups = [], [], []\n",
    "    with torch.no_grad():\n",
    "        # Usiamo un subset per velocizzare la PCA (es. max 5000 immagini)\n",
    "        count = 0\n",
    "        max_images_for_pca = 5000\n",
    "\n",
    "        for batch in tqdm(fairface_loader, desc=\"PCA Immagini\"):\n",
    "            images = batch['pixel_values'].to(device)\n",
    "            image_features = model.get_image_features(images)\n",
    "            image_features_norm = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            all_features.append(image_features_norm.cpu())\n",
    "            all_gender_labels.extend(batch['gender_label'].numpy())\n",
    "            all_race_groups.extend(batch['race_group'])\n",
    "\n",
    "            count += len(images)\n",
    "            if count > max_images_for_pca:\n",
    "                print(f\"Raggiunto limite di {max_images_for_pca} immagini per PCA. Interrompo.\")\n",
    "                break\n",
    "\n",
    "    all_features = torch.cat(all_features).numpy()\n",
    "\n",
    "    if all_features.shape[0] == 0:\n",
    "        print(\"Nessuna immagine trovata per la PCA. Salto il plot.\")\n",
    "        return\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    features_2d = pca.fit_transform(all_features)\n",
    "\n",
    "    df = pd.DataFrame(features_2d, columns=['PC1', 'PC2'])\n",
    "    df['gender'] = ['Female' if g == 1 else 'Male' for g in all_gender_labels]\n",
    "    df['race'] = all_race_groups\n",
    "\n",
    "    # Non c'è più bisogno di campionare, lo abbiamo fatto prima\n",
    "    # if len(df) > 2000: df = df.sample(n=2000, random_state=42)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.scatterplot(data=df, x='PC1', y='PC2', hue='gender', alpha=0.6, s=50)\n",
    "    plt.title(f\"{title_prefix} - PCA Spazio Immagini (Colorato per Genere)\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.6); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(data=df, x='PC1', y='PC2', hue='race', alpha=0.6, s=50)\n",
    "    plt.title(f\"{title_prefix} - PCA Spazio Immagini (Colorato per Etnia)\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2); plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout(); plt.show()\n",
    "    print(\"Plot PCA (Immagini) generati.\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3.6 Esecuzione della Valutazione Baseline\n",
    "# -----------------------------------------------\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # Test 1: Performance Zero-Shot\n",
    "        print(\"\\n--- 3.6.1 Esecuzione Test Zero-Shot (Pets) ---\")\n",
    "        baseline_results[\"pets\"] = evaluate_pets(\n",
    "            model, processor, pets_loader, PROMPTS_PETS, DEVICE\n",
    "        )\n",
    "\n",
    "        # Test 2: Bias di Associazione (Professioni - Pexels Standard)\n",
    "        print(\"\\n--- 3.6.2 Esecuzione Test Bias Associazione (Pexels) ---\")\n",
    "        baseline_results[\"pexels_bias\"] = evaluate_pexels_bias(\n",
    "            model, processor, pexels_loader, CONCEPT_DICTIONARY, DEVICE\n",
    "        )\n",
    "\n",
    "        # Test 3: Bias di Rappresentazione (Genere/Etnia - FairFace)\n",
    "        print(\"\\n--- 3.6.3 Esecuzione Test Rappresentazione (FairFace) ---\")\n",
    "        baseline_results[\"fairface_bias\"] = evaluate_fairface_bias(\n",
    "            model, processor, fairface_loader, PROMPTS_FF_GENDER, PROMPTS_FF_RACE, DEVICE\n",
    "        )\n",
    "\n",
    "        # --- BLOCCO AGGIUNTO ---\n",
    "        print(\"\\n--- 3.6.4 Esecuzione Test Challenge Set (Baseline) ---\")\n",
    "        # (Usa il challenge_loader definito nello STEP 2.6)\n",
    "        baseline_results[\"challenge_bias\"] = evaluate_pexels_bias(\n",
    "            model, processor, challenge_loader, CONCEPT_DICTIONARY, DEVICE\n",
    "        )\n",
    "        # --- FINE BLOCCO AGGIUNTO ---\n",
    "\n",
    "    # Test 4: PCA Spazio Testuale (con salvataggio)\n",
    "    print(\"\\n--- 3.6.5 Esecuzione PCA Spazio Testuale ---\")\n",
    "    plot_text_pca(\n",
    "        model, processor, CONCEPT_DICTIONARY, DEVICE,\n",
    "        title=\"Baseline: PCA Spazio Testuale\",\n",
    "        save_path=OUTPUT_DIR / \"baseline_pca_text.png\"\n",
    "    )\n",
    "\n",
    "    # Test 5: PCA Spazio Immagini (Non serve salvarla, è statica)\n",
    "    print(\"\\n--- 3.6.6 Esecuzione PCA Spazio Immagini ---\")\n",
    "    plot_image_pca(\n",
    "        model, fairface_loader, DEVICE,\n",
    "        title_prefix=\"Baseline\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERRORE DURANTE LA VALUTAZIONE BASELINE: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n--- Step 3 Completato: Valutazione Baseline Eseguita ---\")\n",
    "print(\"\\nRiepilogo Risultati Baseline:\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3.7 Funzione di Conversione e Salvataggio Risultati\n",
    "# -----------------------------------------------\n",
    "\n",
    "# (Questa è la funzione \"intelligente\" che gestisce tutte le chiavi)\n",
    "def convert_results_to_json_serializable(results):\n",
    "    \"\"\"\n",
    "    Converte i risultati dell'esperimento (che contengono oggetti pandas)\n",
    "    in un dizionario serializzabile in JSON.\n",
    "    Gestisce automaticamente tutte le chiavi di tipo 'bias'.\n",
    "    \"\"\"\n",
    "    results_copy = results.copy()\n",
    "\n",
    "    # Lista di chiavi che hanno lo stesso formato di 'pexels_bias'\n",
    "    pexels_style_keys = ['pexels_bias', 'pexels_bias_holdout', 'challenge_bias']\n",
    "\n",
    "    for key in pexels_style_keys:\n",
    "        if key in results_copy and results_copy[key] is not None:\n",
    "            # Controllo per assicurarsi che 'grouped_bias' non sia una Serie vuota\n",
    "            if not results_copy[key]['grouped_bias'].empty:\n",
    "                grouped_data = results_copy[key]['grouped_bias'].reset_index().to_dict('records')\n",
    "            else:\n",
    "                grouped_data = [] # Salva come lista vuota\n",
    "\n",
    "            results_copy[key] = {\n",
    "                'grouped_bias_records': grouped_data,\n",
    "                'overall_abs_bias': results_copy[key]['overall_abs_bias']\n",
    "            }\n",
    "\n",
    "    # Gestione FairFace (resta invariato)\n",
    "    if 'fairface_bias' in results_copy and results_copy['fairface_bias'] is not None:\n",
    "        results_copy['fairface_bias'] = results_copy['fairface_bias'].copy()\n",
    "        if 'gender_acc_by_race' in results_copy['fairface_bias'] and not results_copy['fairface_bias']['gender_acc_by_race'].empty:\n",
    "            results_copy['fairface_bias']['gender_acc_by_race'] = results_copy['fairface_bias']['gender_acc_by_race'].to_dict()\n",
    "        else:\n",
    "            results_copy['fairface_bias']['gender_acc_by_race'] = {} # Salva come dizionario vuoto\n",
    "\n",
    "        if 'race_acc_by_race' in results_copy['fairface_bias'] and not results_copy['fairface_bias']['race_acc_by_race'].empty:\n",
    "            results_copy['fairface_bias']['race_acc_by_race'] = results_copy['fairface_bias']['race_acc_by_race'].to_dict()\n",
    "        else:\n",
    "             results_copy['fairface_bias']['race_acc_by_race'] = {} # Salva come dizionario vuoto\n",
    "\n",
    "    return results_copy\n",
    "\n",
    "# Stampa e salva i risultati\n",
    "serializable_baseline_results = convert_results_to_json_serializable(baseline_results)\n",
    "print(json.dumps(serializable_baseline_results, indent=2))\n",
    "\n",
    "print(\"\\n--- 3.7 Salvataggio Risultati Baseline su Drive ---\")\n",
    "baseline_save_path = OUTPUT_DIR / \"baseline_results.json\"\n",
    "try:\n",
    "    # MODIFICA: Aggiungiamo 'challenge_bias' al dizionario\n",
    "    results_to_save = {\n",
    "        \"pets\": serializable_baseline_results.get(\"pets\", {}),\n",
    "        \"fairface_bias\": serializable_baseline_results.get(\"fairface_bias\", {}),\n",
    "        \"pexels_bias\": serializable_baseline_results.get(\"pexels_bias\", {}),\n",
    "        \"challenge_bias\": serializable_baseline_results.get(\"challenge_bias\", {}) # <-- RIGA AGGIUNTA\n",
    "    }\n",
    "    with open(baseline_save_path, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=4)\n",
    "    print(f\"Risultati Pets, FairFace, Pexels e Challenge (Baseline) salvati in: {baseline_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante il salvataggio dei risultati baseline: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAbSgJYlKnoK"
   },
   "source": [
    "# FineTuning Setup\n",
    "- LoRa\n",
    "- Loss Definition\n",
    "- Utilis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUr8JyCoK6T3"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# STEP 4: PREPARAZIONE PER IL FINETUNING\n",
    "# (NUOVO CODICE)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"INIZIO STEP 4: PREPARAZIONE FINETUNING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 4.1 Calcolo degli \"Ancoraggi\" (Mean Embeddings)\n",
    "# (Come da nostra discussione)\n",
    "# -----------------------------------------------\n",
    "def calculate_anchors(model, processor, concept_dict, device):\n",
    "    \"\"\"\n",
    "    Calcola gli embedding di ancoraggio come media dei template di training.\n",
    "    \"\"\"\n",
    "    print(\"Inizio calcolo Ancoraggi (Mean Embeddings)...\")\n",
    "    model.eval() # Assicurati di essere in modalità eval\n",
    "    anchors = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for prof in concept_dict['professions']:\n",
    "            # 1. Crea tutti i prompt di training per questa professione\n",
    "            prompts = [template.format(prof) for template in concept_dict['train_templates']]\n",
    "\n",
    "            # 2. Codifica tutti i prompt in un batch\n",
    "            inputs = processor(text=prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "            text_features = model.get_text_features(**inputs)\n",
    "\n",
    "            # 3. Normalizza gli embedding\n",
    "            text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # 4. Calcola la media e salva\n",
    "            mean_anchor = text_features_norm.mean(dim=0)\n",
    "\n",
    "            # 5. Normalizza di nuovo la media (importante!)\n",
    "            anchors[prof] = mean_anchor / mean_anchor.norm()\n",
    "\n",
    "            print(f\"Ancoraggio calcolato per: {prof}\")\n",
    "\n",
    "    print(\"Calcolo ancoraggi completato.\")\n",
    "    return anchors\n",
    "\n",
    "# Calcoliamo gli ancoraggi usando il modello *originale*\n",
    "anchors = calculate_anchors(model, processor, CONCEPT_DICTIONARY, DEVICE)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 4.2 Configurazione di LoRA (PEFT)\n",
    "# (Come da nostra discussione)\n",
    "# -----------------------------------------------\n",
    "print(\"Configurazione di LoRA (PEFT) sul Text Encoder...\")\n",
    "\n",
    "# 1. Definiamo la configurazione LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    # Applichiamo LoRA a tutti i layer di proiezione dell'Attention\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# 2. Congeliamo esplicitamente il Vision Transformer (buona prassi)\n",
    "for param in model.vision_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Vision Transformer (ViT) congelato.\")\n",
    "\n",
    "# 3. Applichiamo LoRA *solo* al Text Encoder\n",
    "model.text_model = get_peft_model(model.text_model, lora_config)\n",
    "print(\"LoRA applicato al Text Encoder.\")\n",
    "\n",
    "# 4. Stampa i parametri addestrabili per conferma\n",
    "model.text_model.print_trainable_parameters() # <-- CORRETTO\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 4.3 Definizione Iperparametri e Ottimizzatore\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Iperparametri\n",
    "NUM_STEPS = 1500\n",
    "LEARNING_RATE = 1e-5  # Come da piano (1e-5 o 5e-6)\n",
    "LAMBDA_ANCHOR = 0.05   # Peso della anchor loss (critico)\n",
    "DEBIAS_MARGIN = 0.1   # Margine per la L_debias (come da accordo)\n",
    "\n",
    "# Definiamo l'ottimizzatore\n",
    "# Passiamo solo i parametri del modello. PEFT si occupa di\n",
    "# restituire solo quelli addestrabili (i pesi LoRA).\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Iperparametri impostati:\")\n",
    "print(f\"  - STEPS: {NUM_STEPS}\")\n",
    "print(f\"  - LR: {LEARNING_RATE}\")\n",
    "print(f\"  - LAMBDA_ANCHOR: {LAMBDA_ANCHOR}\")\n",
    "print(f\"  - DEBIAS_MARGIN: {DEBIAS_MARGIN}\")\n",
    "print(\"Ottimizzatore AdamW creato.\")\n",
    "# -----------------------------------------------\n",
    "# 4.4 Tentativo di Caricamento Pesi Esistenti\n",
    "# (NUOVO BLOCCO AGGIUNTO)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n--- 4.4 Definizione Path e Tentativo di Caricamento Adattatori ---\")\n",
    "\n",
    "# Definiamo il path qui, così è disponibile sia per il loading (ora)\n",
    "# che per il saving (nello Step 5)\n",
    "ADAPTER_SAVE_PATH = OUTPUT_DIR / \"finetuned_clip_lora_adapters\"\n",
    "\n",
    "skip_training = False # Flag per saltare il training se i pesi esistono\n",
    "\n",
    "try:\n",
    "    # Prova a caricare i pesi.\n",
    "    # Applichiamo i pesi LoRA al text_model, perché è quello che\n",
    "    # abbiamo configurato per il training\n",
    "    model.text_model = PeftModel.from_pretrained(model.text_model, ADAPTER_SAVE_PATH)\n",
    "\n",
    "    # Se il caricamento riesce:\n",
    "    skip_training = True\n",
    "    model.eval() # Mettiamo il modello in modalità valutazione\n",
    "    print(f\"!!! SUCCESSO: Pesi LoRA pre-addestrati trovati e caricati da: {ADAPTER_SAVE_PATH}\")\n",
    "    print(\"--- Verrà saltato il TRAINING (STEP 5) ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    # L'eccezione più comune è OSError se la cartella non esiste\n",
    "    print(f\"Pesi LoRA non trovati in {ADAPTER_SAVE_PATH} (Errore: {e})\")\n",
    "    print(\"Si procederà con il training normale (STEP 5).\")\n",
    "\n",
    "print(\"\\n--- Step 4 Completato: Preparazione Finetuning ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHdFyVU8Q480"
   },
   "source": [
    "# TRAINING LOOP EXECUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNfEWWpwRCZb"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# STEP 5: ESECUZIONE DEL FINETUNING\n",
    "# (MODIFICATO per saltare se i pesi sono stati caricati)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"INIZIO STEP 5: ESECUZIONE FINETUNING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Inizializziamo la history qui, così esiste in ogni caso\n",
    "loss_history = {\n",
    "    'total': [],\n",
    "    'debias': [],\n",
    "    'anchor': []\n",
    "}\n",
    "\n",
    "# Controlliamo il flag impostato nello STEP 4\n",
    "# La variabile 'skip_training' e 'ADAPTER_SAVE_PATH' dovrebbero essere\n",
    "# state definite alla fine dello STEP 4\n",
    "if skip_training:\n",
    "    print(\"STEP 5 SKIPPATO: I pesi del modello sono già stati caricati.\")\n",
    "\n",
    "else:\n",
    "    # --- Se i pesi non sono stati trovati, esegui il training ---\n",
    "    print(\"Avvio del training loop...\")\n",
    "\n",
    "    # Mettiamo il modello in modalità training (attiva i dropout LoRA)\n",
    "    model.train()\n",
    "\n",
    "    # Iniziamo il training loop\n",
    "    pbar = tqdm(range(NUM_STEPS), desc=\"Finetuning Steps\")\n",
    "    for step in pbar:\n",
    "\n",
    "        # --- 1. Campionamento Casuale ---\n",
    "        prof = random.choice(CONCEPT_DICTIONARY['professions'])\n",
    "        g_pair = random.choice(CONCEPT_DICTIONARY['gender_pairs'])\n",
    "        template = random.choice(CONCEPT_DICTIONARY['train_templates'])\n",
    "\n",
    "        # --- 2. Preparazione Dati per lo Step ---\n",
    "        # Prendiamo l'ancoraggio pre-calcolato\n",
    "        anchor_embed = anchors[prof].to(DEVICE)\n",
    "\n",
    "        # Creiamo i prompt\n",
    "        prompt_p = template.format(prof)\n",
    "        prompt_g1 = template.format(g_pair[0]) # es. \"a photo of a man\"\n",
    "        prompt_g2 = template.format(g_pair[1]) # es. \"a photo of a woman\"\n",
    "\n",
    "        # --- 3. Calcolo Embedding Correnti ---\n",
    "        # Codifichiamo tutto in un unico batch\n",
    "        inputs = processor(\n",
    "            text=[prompt_p, prompt_g1, prompt_g2],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Otteniamo gli embedding dal text encoder (che ora ha LoRA)\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "        text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Separiamo gli embedding\n",
    "        current_embed_p = text_features_norm[0]\n",
    "        embed_g1 = text_features_norm[1]\n",
    "        embed_g2 = text_features_norm[2]\n",
    "\n",
    "        # --- 4. Calcolo Loss Function Doppia ---\n",
    "\n",
    "        # L_debias (Equidistanza con Margin Loss)\n",
    "        sim_p_g1 = cosine_similarity(current_embed_p.unsqueeze(0), embed_g1.unsqueeze(0)).squeeze()\n",
    "        sim_p_g2 = cosine_similarity(current_embed_p.unsqueeze(0), embed_g2.unsqueeze(0)).squeeze()\n",
    "\n",
    "        # L_debias = max(0, abs(sim1 - sim2) - MARGIN)^2\n",
    "        L_debias = (sim_p_g1 - sim_p_g2)**2\n",
    "\n",
    "        # L_anchor (Anti-Drift)\n",
    "        # Vogliamo massimizzare la similarità, quindi minimizziamo (1 - similarità)\n",
    "        L_anchor = 1.0 - cosine_similarity(current_embed_p.unsqueeze(0), anchor_embed.unsqueeze(0)).squeeze()\n",
    "\n",
    "        # L_total\n",
    "        L_total = L_debias + LAMBDA_ANCHOR * L_anchor\n",
    "\n",
    "        # --- 5. Backpropagation ---\n",
    "        optimizer.zero_grad()\n",
    "        L_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- 6. Logging ---\n",
    "        loss_history['total'].append(L_total.item())\n",
    "        loss_history['debias'].append(L_debias.item())\n",
    "        loss_history['anchor'].append(L_anchor.item())\n",
    "\n",
    "        if step % 100 == 0 or step == NUM_STEPS - 1:\n",
    "            avg_total = np.mean(loss_history['total'][-100:])\n",
    "            avg_debias = np.mean(loss_history['debias'][-100:])\n",
    "            avg_anchor = np.mean(loss_history['anchor'][-100:])\n",
    "            pbar.set_description(\n",
    "                f\"Step {step} | L_total: {avg_total:.4f} (L_debias: {avg_debias:.4f}, L_anchor: {avg_anchor:.4f})\"\n",
    "            )\n",
    "\n",
    "    print(\"Finetuning completato.\")\n",
    "\n",
    "    # Plot delle loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(loss_history['total'], label='L_total', alpha=0.8)\n",
    "    plt.plot(loss_history['debias'], label='L_debias', linestyle='--')\n",
    "    plt.plot(loss_history['anchor'], label=f'L_anchor (x{LAMBDA_ANCHOR})', linestyle=':')\n",
    "    plt.title(\"Andamento delle Loss durante il Training\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # --- BLOCCO SALVATAGGIO GRAFICO LOSS ---\n",
    "    loss_plot_path = OUTPUT_DIR / \"finetuning_loss_history.png\"\n",
    "    try:\n",
    "        plt.savefig(loss_plot_path, bbox_inches='tight')\n",
    "        print(f\"Grafico Loss salvato in: {loss_plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore salvataggio grafico loss: {e}\")\n",
    "    # --- FINE BLOCCO ---\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # --- BLOCCO SALVATAGGIO DATI LOSS (JSON) ---\n",
    "    print(\"\\n--- 5.1 Salvataggio Dati Loss History su Drive ---\")\n",
    "    loss_data_path = OUTPUT_DIR / \"finetuning_loss_history.json\"\n",
    "    try:\n",
    "        with open(loss_data_path, 'w') as f:\n",
    "            json.dump(loss_history, f, indent=4)\n",
    "        print(f\"Dati della Loss History salvati in: {loss_data_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il salvataggio dei dati di loss: {e}\")\n",
    "    # --- FINE BLOCCO ---\n",
    "\n",
    "    # --- BLOCCO SALVATAGGIO PESI MODELLO (ADATTATORI LORA) ---\n",
    "    print(\"\\n--- 5.2 Salvataggio Pesi del Modello (Adattatori LoRA) ---\")\n",
    "    # (Usiamo ADAPTER_SAVE_PATH definito nello Step 4.4)\n",
    "    try:\n",
    "        # Salviamo solo i pesi addestrati (adattatori LoRA)\n",
    "        model.save_pretrained(ADAPTER_SAVE_PATH)\n",
    "\n",
    "        # Salviamo anche la config del text_model per coerenza\n",
    "        model.config.text_config.save_pretrained(ADAPTER_SAVE_PATH)\n",
    "\n",
    "        print(f\"Adattatori LoRA salvati con successo in: {ADAPTER_SAVE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore during il salvataggio degli adattatori del modello: {e}\")\n",
    "    # --- FINE BLOCCO ---\n",
    "\n",
    "print(\"\\n--- Step 5 Completato ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLamDKVhRDIH"
   },
   "source": [
    "# POST FINE-TUNING EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftpE_GV0RWoW"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# STEP 6: VALUTAZIONE FINALE (POST-FINETUNING)\n",
    "# (MODIFICATO per includere il Challenge Set)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"INIZIO STEP 6: VALUTAZIONE POST-FINETUNING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "post_results = {}\n",
    "model.eval()\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # --- Test 1: Performance Zero-Shot (Pets) ---\n",
    "        print(\"\\n--- 6.1 Esecuzione Test Zero-Shot (Pets) Post-Finetuning ---\")\n",
    "        post_results[\"pets\"] = evaluate_pets(\n",
    "            model, processor, pets_loader, PROMPTS_PETS, DEVICE\n",
    "        )\n",
    "\n",
    "        # --- Test 2: Bias di Associazione (Pexels - Template Standard) ---\n",
    "        print(\"\\n--- 6.2 Esecuzione Test Bias Associazione (Pexels) Post-Finetuning ---\")\n",
    "        post_results[\"pexels_bias\"] = evaluate_pexels_bias(\n",
    "            model, processor, pexels_loader, CONCEPT_DICTIONARY, DEVICE,\n",
    "            eval_template=\"a photo of a {} {}\"\n",
    "        )\n",
    "\n",
    "        # --- Test 3: Bias di Rappresentazione (FairFace) ---\n",
    "        print(\"\\n--- 6.3 Esecuzione Test Rappresentazione (FairFace) Post-Finetuning ---\")\n",
    "        post_results[\"fairface_bias\"] = evaluate_fairface_bias(\n",
    "            model, processor, fairface_loader, PROMPTS_FF_GENDER, PROMPTS_FF_RACE, DEVICE\n",
    "        )\n",
    "\n",
    "        # --- Test 4: Generalizzazione (Pexels - Template Hold-out) ---\n",
    "        print(\"\\n--- 6.4 Esecuzione Test Generalizzazione (Pexels Hold-out) Post-Finetuning ---\")\n",
    "        holdout_template = \"a high-quality image of a {} {}\"\n",
    "        post_results[\"pexels_bias_holdout\"] = evaluate_pexels_bias(\n",
    "            model, processor, pexels_loader, CONCEPT_DICTIONARY, DEVICE,\n",
    "            eval_template=holdout_template\n",
    "        )\n",
    "\n",
    "        # --- BLOCCO AGGIUNTO ---\n",
    "        print(\"\\n--- 6.5 Esecuzione Test Challenge Set (Post-Finetuning) ---\")\n",
    "        # (Usa il challenge_loader definito nello STEP 2.6)\n",
    "        post_results[\"challenge_bias\"] = evaluate_pexels_bias(\n",
    "            model, processor, challenge_loader, CONCEPT_DICTIONARY, DEVICE\n",
    "        )\n",
    "        # --- FINE BLOCCO AGGIUNTO ---\n",
    "\n",
    "    # --- Test 6: PCA Spazio Testuale (Post) (con salvataggio) ---\n",
    "    print(\"\\n--- 6.6 Esecuzione PCA Spazio Testuale Post-Finetuning ---\")\n",
    "    plot_text_pca(\n",
    "        model, processor, CONCEPT_DICTIONARY, DEVICE,\n",
    "        title=\"Post-Finetuning: PCA Spazio Testuale\",\n",
    "        save_path=OUTPUT_DIR / \"post_finetuning_pca_text.png\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERRORE DURANTE LA VALUTAZIONE POST-FINETUNING: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n--- Step 6 Completato: Valutazione Post-Finetuning Eseguita ---\")\n",
    "print(\"\\nRiepilogo Risultati Post-Finetuning:\")\n",
    "\n",
    "# (Assicurati che la funzione 'convert_results_to_json_serializable' sia definita nello Step 3)\n",
    "serializable_post_results = convert_results_to_json_serializable(post_results)\n",
    "print(json.dumps(serializable_post_results, indent=2))\n",
    "\n",
    "# --- 6.7 Salvataggio Risultati Post-Finetuning su Drive ---\n",
    "\n",
    "print(\"\\n--- 6.7 Salvataggio Risultati Post-Finetuning su Drive ---\")\n",
    "post_save_path = OUTPUT_DIR / \"post_finetuning_results.json\"\n",
    "try:\n",
    "    # MODIFICA: Aggiungiamo 'challenge_bias'\n",
    "    results_to_save = {\n",
    "        \"pets\": serializable_post_results.get(\"pets\", {}),\n",
    "        \"fairface_bias\": serializable_post_results.get(\"fairface_bias\", {}),\n",
    "        \"pexels_bias\": serializable_post_results.get(\"pexels_bias\", {}),\n",
    "        \"pexels_bias_holdout\": serializable_post_results.get(\"pexels_bias_holdout\", {}),\n",
    "        \"challenge_bias\": serializable_post_results.get(\"challenge_bias\", {}) # <-- RIGA AGGIUNTA\n",
    "    }\n",
    "    with open(post_save_path, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=4)\n",
    "    print(f\"Risultati Pets, FairFace, Pexels e Challenge (Post-Finetuning) salvati in: {post_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante il salvataggio dei risultati post-finetuning: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YxL8l_8RXn8"
   },
   "source": [
    "# Comparation PRE POST FINE-TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTg5h7riRN4-"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# STEP 7: ANALISI COMPARATIVA E CONCLUSIONI\n",
    "# (MODIFICATO per includere il Challenge Set ovunque)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"INIZIO STEP 7: ANALISI COMPARATIVA (PRE vs POST)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 7.1 Confronto Performance Zero-Shot (OxfordIIITPet)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n--- 7.1 Confronto Performance Zero-Shot (Pets) ---\")\n",
    "try:\n",
    "    pets_data = {\n",
    "        'Baseline': baseline_results['pets'],\n",
    "        'Post-Finetune': post_results['pets']\n",
    "    }\n",
    "    pets_df = pd.DataFrame(pets_data).T\n",
    "    print(pets_df)\n",
    "\n",
    "    pets_df.plot(\n",
    "        kind='bar',\n",
    "        title='Confronto Accuratezza Zero-Shot (OxfordIIITPet)',\n",
    "        figsize=(8, 5)\n",
    "    )\n",
    "    plt.ylabel('Accuratezza')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.ylim(bottom=max(0, pets_df.min().min() - 0.1)) # Imposta un limite y ragionevole\n",
    "    plt.savefig(OUTPUT_DIR / \"comparison_pets_accuracy.png\", bbox_inches='tight')\n",
    "    print(f\"Grafico Pets salvato in: {OUTPUT_DIR / 'comparison_pets_accuracy.png'}\")\n",
    "    plt.show()\n",
    "except KeyError:\n",
    "    print(\"Dati 'pets' non trovati. Salto del grafico.\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 7.2 Confronto Bias di Associazione (Pexels)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n--- 7.2 Confronto Bias di Associazione (Pexels) ---\")\n",
    "try:\n",
    "    bias_data = {\n",
    "        'Baseline (Template Standard)': baseline_results['pexels_bias']['overall_abs_bias'],\n",
    "        'Post-Finetune (Template Standard)': post_results['pexels_bias']['overall_abs_bias'],\n",
    "        'Post-Finetune (Template Hold-out)': post_results['pexels_bias_holdout']['overall_abs_bias']\n",
    "    }\n",
    "    bias_df = pd.DataFrame.from_dict(bias_data, orient='index', columns=['Overall Absolute Bias'])\n",
    "    print(bias_df)\n",
    "\n",
    "    bias_df.plot(\n",
    "        kind='bar',\n",
    "        title='Confronto Bias di Associazione (Pexels)',\n",
    "        figsize=(10, 6),\n",
    "        legend=False\n",
    "    )\n",
    "    plt.ylabel('Overall Absolute Bias Score (Vicino a 0 è meglio)')\n",
    "    plt.xticks(rotation=15, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(OUTPUT_DIR / \"comparison_pexels_bias.png\", bbox_inches='tight')\n",
    "    print(f\"Grafico Pexels Bias salvato in: {OUTPUT_DIR / 'comparison_pexels_bias.png'}\")\n",
    "    plt.show()\n",
    "except KeyError:\n",
    "    print(\"Dati 'pexels_bias' non trovati. Salto del grafico.\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 7.3 Confronto Bias di Rappresentazione (FairFace)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n--- 7.3 Confronto Bias di Rappresentazione (FairFace) ---\")\n",
    "try:\n",
    "    baseline_ff = baseline_results['fairface_bias']['gender_acc_by_race']\n",
    "    post_ff = post_results['fairface_bias']['gender_acc_by_race']\n",
    "\n",
    "    ff_df = pd.DataFrame({\n",
    "        'Baseline': baseline_ff,\n",
    "        'Post-Finetune': post_ff\n",
    "    })\n",
    "    print(ff_df)\n",
    "\n",
    "    ff_df.plot(\n",
    "        kind='bar',\n",
    "        title='Accuratezza Classificazione Genere per Etnia (FairFace)',\n",
    "        figsize=(12, 7)\n",
    "    )\n",
    "    plt.ylabel('Accuratezza')\n",
    "    plt.xlabel('Gruppo Etnico (FairFace)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.ylim(bottom=max(0, ff_df.min().min() - 0.1))\n",
    "    plt.savefig(OUTPUT_DIR / \"comparison_fairface_accuracy.png\", bbox_inches='tight')\n",
    "    print(f\"Grafico FairFace Accuracy salvato in: {OUTPUT_DIR / 'comparison_fairface_accuracy.png'}\")\n",
    "    plt.show()\n",
    "except KeyError:\n",
    "    print(\"Dati 'fairface_bias' non trovati. Salto del grafico.\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 7.4 Confronto Association Bias (Challenge Set)\n",
    "# -----------------------------------------------\n",
    "print(\"\\n--- 7.4 Confronto Bias di Associazione (Challenge Set) ---\")\n",
    "\n",
    "try:\n",
    "    challenge_bias_data = {\n",
    "        'Baseline (Challenge Set)': baseline_results['challenge_bias']['overall_abs_bias'],\n",
    "        'Post-Finetune (Challenge Set)': post_results['challenge_bias']['overall_abs_bias']\n",
    "    }\n",
    "    challenge_bias_df = pd.DataFrame.from_dict(challenge_bias_data, orient='index', columns=['Overall Absolute Bias'])\n",
    "    print(challenge_bias_df)\n",
    "\n",
    "    challenge_bias_df.plot(\n",
    "        kind='bar',\n",
    "        title='Confronto Bias di Associazione (Challenge Set Ambigue)',\n",
    "        figsize=(8, 5),\n",
    "        legend=False\n",
    "    )\n",
    "    plt.ylabel('Overall Absolute Bias Score (Vicino a 0 è meglio)')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.savefig(OUTPUT_DIR / \"comparison_challenge_bias.png\", bbox_inches='tight')\n",
    "    print(f\"Grafico Challenge Bias salvato in: {OUTPUT_DIR / 'comparison_challenge_bias.png'}\")\n",
    "    plt.show()\n",
    "\n",
    "except KeyError:\n",
    "    print(\"Dati del 'challenge_bias' non trovati. Salto del grafico.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 7.5 Conclusioni\n",
    "# -----------------------------------------------\n",
    "print(\"\\n--- 7.5 Conclusioni ---\")\n",
    "# Calcoliamo le differenze\n",
    "try:\n",
    "    # Pets\n",
    "    pet_top1_diff = post_results['pets']['top1_accuracy'] - baseline_results['pets']['top1_accuracy']\n",
    "\n",
    "    # Pexels\n",
    "    bias_diff = post_results['pexels_bias']['overall_abs_bias'] - baseline_results['pexels_bias']['overall_abs_bias']\n",
    "    if baseline_results['pexels_bias']['overall_abs_bias'] > 0:\n",
    "        bias_perc_reduction = (-bias_diff / baseline_results['pexels_bias']['overall_abs_bias']) * 100\n",
    "    else:\n",
    "        bias_perc_reduction = float('inf') if bias_diff < 0 else 0.0\n",
    "\n",
    "    # FairFace\n",
    "    ff_gender_diff = post_results['fairface_bias']['overall_gender_acc'] - baseline_results['fairface_bias']['overall_gender_acc']\n",
    "\n",
    "    # --- BLOCCO AGGIUNTO ---\n",
    "    # Challenge Set\n",
    "    challenge_bias_diff = post_results['challenge_bias']['overall_abs_bias'] - baseline_results['challenge_bias']['overall_abs_bias']\n",
    "    if baseline_results['challenge_bias']['overall_abs_bias'] > 0:\n",
    "        challenge_bias_perc_reduction = (-challenge_bias_diff / baseline_results['challenge_bias']['overall_abs_bias']) * 100\n",
    "    else:\n",
    "        challenge_bias_perc_reduction = float('inf') if challenge_bias_diff < 0 else 0.0\n",
    "    # --- FINE BLOCCO AGGIUNTO ---\n",
    "\n",
    "    print(\"Analisi Quantitativa (PRE vs POST):\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(f\"🎯 Performance Zero-Shot (Pets):\")\n",
    "    print(f\"  - Baseline Top-1: {baseline_results['pets']['top1_accuracy']:.4f}\")\n",
    "    print(f\"  - Post-Finetune Top-1: {post_results['pets']['top1_accuracy']:.4f}\")\n",
    "    print(f\"  - Differenza: {pet_top1_diff:+.4f} (Obiettivo: > -0.02)\")\n",
    "\n",
    "    print(f\"\\n🎯 Bias di Associazione (Pexels - Immagini Facili):\")\n",
    "    print(f\"  - Baseline Bias Assoluto: {baseline_results['pexels_bias']['overall_abs_bias']:.4f}\")\n",
    "    print(f\"  - Post-Finetune Bias Assoluto: {post_results['pexels_bias']['overall_abs_bias']:.4f}\")\n",
    "    print(f\"  - Riduzione del Bias: {bias_perc_reduction:.2f}% (Obiettivo: Alto)\")\n",
    "\n",
    "    print(f\"\\n🎯 Bias di Associazione (Challenge Set - Immagini Ambigue):\") # <-- BLOCCO AGGIUNTO\n",
    "    print(f\"  - Baseline Bias Assoluto: {baseline_results['challenge_bias']['overall_abs_bias']:.4f}\")\n",
    "    print(f\"  - Post-Finetune Bias Assoluto: {post_results['challenge_bias']['overall_abs_bias']:.4f}\")\n",
    "    print(f\"  - Riduzione del Bias: {challenge_bias_perc_reduction:.2f}% (Obiettivo: Alto)\")\n",
    "\n",
    "    print(f\"\\n🎯 Generalizzazione (Pexels Hold-out):\")\n",
    "    print(f\"  - Bias Post (Standard): {post_results['pexels_bias']['overall_abs_bias']:.4f}\")\n",
    "    print(f\"  - Bias Post (Hold-out): {post_results['pexels_bias_holdout']['overall_abs_bias']:.4f}\")\n",
    "    if post_results['pexels_bias_holdout']['overall_abs_bias'] < baseline_results['pexels_bias']['overall_abs_bias']:\n",
    "        print(\"  - ✔️ Successo: Il debiasing si è generalizzato a nuovi template.\")\n",
    "    else:\n",
    "        print(\"  - ⚠️ Attenzione: Overfitting sui template di training?\")\n",
    "\n",
    "    print(f\"\\n🎯 Bias di Rappresentazione (FairFace):\")\n",
    "    print(f\"  - Baseline Acc. Genere: {baseline_results['fairface_bias']['overall_gender_acc']:.4f}\")\n",
    "    print(f\"  - Post-Finetune Acc. Genere: {post_results['fairface_bias']['overall_gender_acc']:.4f}\")\n",
    "    print(f\"  - Differenza: {ff_gender_diff:+.4f} (Obiettivo: ~0.0)\")\n",
    "\n",
    "    print(\"\\nVisualizzazioni PCA:\")\n",
    "    print(\"  - Confronta i due plot 'PCA Spazio Testuale' (Step 3 e 6).\")\n",
    "    print(\"  - Ci aspettiamo di vedere 'engineer', 'nurse', 'doctor' muoversi verso\")\n",
    "    print(\"    una posizione più centrale rispetto agli assi di genere.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Errore nella generazione delle conclusioni quantitative: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SCRIPT COMPLETO ESEGUITO\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNvfodDmT0o8yHogV4wcGkn",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1rHL5wL5crzvQCl92Fz1vkbSGjoEUo-av",
     "timestamp": 1761188162112
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
